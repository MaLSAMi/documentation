{
    "docs": [
        {
            "location": "/",
            "text": "MaLSAMi project documentation\n\u00b6\n\n\nWelcome to the MaLSAMi project documentation.\n\n\nIn here we will document our progress and almost all subprojects or programs\nwe will create or write.\nIf you want to take a deeper look in our open source code projects please refer to the \nMaLSAMi Github Page\n\n\nAbout the project\n\u00b6\n\n\nYou can find a general description of the project MaLSAMi in the \nAbout Section\n\n\nPrerequisite\n\u00b6\n\n\nTo contribute to this documentation please download the source code from the \nMaLSAMi Documentation Repository\n\nand follow the instructions at \nMkDocs\n.\n\n\nDocumentation structure\n\u00b6\n\n\nThe documentation is structured into different subpages which are representing subprojects of the project MaLSAMi.\nNevertheless also analysis results or specifications of the projects can be described in a subpage.\n\n\nCommands\n\u00b6\n\n\nTo work with MkDocs after installing it, you can use these commands\n\n\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nTo upload the documentation to the git repository please following command, but be aware that changes might be immediately visible.\n\n\n\n\nmkdocs gh-deploy\n - Uploads build documentation sites to gh-pages branch\n\n\n\n\nProject layout\n\u00b6\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "Home"
        },
        {
            "location": "/#malsami-project-documentation",
            "text": "Welcome to the MaLSAMi project documentation.  In here we will document our progress and almost all subprojects or programs\nwe will create or write.\nIf you want to take a deeper look in our open source code projects please refer to the  MaLSAMi Github Page",
            "title": "MaLSAMi project documentation"
        },
        {
            "location": "/#about-the-project",
            "text": "You can find a general description of the project MaLSAMi in the  About Section",
            "title": "About the project"
        },
        {
            "location": "/#prerequisite",
            "text": "To contribute to this documentation please download the source code from the  MaLSAMi Documentation Repository \nand follow the instructions at  MkDocs .",
            "title": "Prerequisite"
        },
        {
            "location": "/#documentation-structure",
            "text": "The documentation is structured into different subpages which are representing subprojects of the project MaLSAMi.\nNevertheless also analysis results or specifications of the projects can be described in a subpage.",
            "title": "Documentation structure"
        },
        {
            "location": "/#commands",
            "text": "To work with MkDocs after installing it, you can use these commands   mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.   To upload the documentation to the git repository please following command, but be aware that changes might be immediately visible.   mkdocs gh-deploy  - Uploads build documentation sites to gh-pages branch",
            "title": "Commands"
        },
        {
            "location": "/#project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "Project layout"
        },
        {
            "location": "/first-steps/",
            "text": "First Steps\n\u00b6\n\n\nTo try our setup you first have to checkout the client-tools repo\n\n\ngit clone https://github.com/malsami/client-tools\n\n\n\n\nfrom here on out you can use the Makefile in the client-tools repository.\nSwitch into the folder:\n\n\ncd client-tools\n\n\n\n\nTest Setup (Vagrant)\n\u00b6\n\n\nIf you simply want to test the setup inside a small vagrant machine, type:\n\n\nmake\n\n\n\n\nthis will take a while, but leave you inside a vagrant machine. \n\n\nHere you can go to the distributor component and run a test program.\n\n\ncd /vagrant/distributor/\nsudo python3 test.py\n\n\n\n\nOnly Genode Image and Task Binaries\n\u00b6\n\n\nIf you only want to build the operating system (for the target focnados_pbxa0) including the binaries execute\nIf you want another target adjust the parameter accordingly. The binaries can afterwards be found in the \u2018bin\u2019 folder.\n\n\nmake genode-init\nmake binaries OS-TARGET=focnados_pbxa9\n\n\n\n\nLocal Execution\n\u00b6\n\n\nFor local execution a dhcp server has to be setup which takes care of ip assignment.\nIf the execution applies Genode on Qemu instances sudo rights are necessary to create and remove the network interfaces.\nIf the execution uses Genode on real hardware, the boards have to be prepared accordingly and the images must be provided for them to fetch.\nTo setup the repository execute (adjust OS-TARGET accordingly)\n\n\nmake venv\nmake distributor-init\nmake genode-init\nmake binaries OS-TARGET=focnados_pbxa9\nmake datageneration\n\n\n\n\nThen you can enter the venv by typing \n\n\nsource malsami/bin/activate\n\n\n\n\nYou are now inside the venv within which the dependencies are ready to execute a programm like e.g. the main.py inside the datageneration.",
            "title": "First Steps"
        },
        {
            "location": "/first-steps/#first-steps",
            "text": "To try our setup you first have to checkout the client-tools repo  git clone https://github.com/malsami/client-tools  from here on out you can use the Makefile in the client-tools repository.\nSwitch into the folder:  cd client-tools",
            "title": "First Steps"
        },
        {
            "location": "/first-steps/#test-setup-vagrant",
            "text": "If you simply want to test the setup inside a small vagrant machine, type:  make  this will take a while, but leave you inside a vagrant machine.   Here you can go to the distributor component and run a test program.  cd /vagrant/distributor/\nsudo python3 test.py",
            "title": "Test Setup (Vagrant)"
        },
        {
            "location": "/first-steps/#only-genode-image-and-task-binaries",
            "text": "If you only want to build the operating system (for the target focnados_pbxa0) including the binaries execute\nIf you want another target adjust the parameter accordingly. The binaries can afterwards be found in the \u2018bin\u2019 folder.  make genode-init\nmake binaries OS-TARGET=focnados_pbxa9",
            "title": "Only Genode Image and Task Binaries"
        },
        {
            "location": "/first-steps/#local-execution",
            "text": "For local execution a dhcp server has to be setup which takes care of ip assignment.\nIf the execution applies Genode on Qemu instances sudo rights are necessary to create and remove the network interfaces.\nIf the execution uses Genode on real hardware, the boards have to be prepared accordingly and the images must be provided for them to fetch.\nTo setup the repository execute (adjust OS-TARGET accordingly)  make venv\nmake distributor-init\nmake genode-init\nmake binaries OS-TARGET=focnados_pbxa9\nmake datageneration  Then you can enter the venv by typing   source malsami/bin/activate  You are now inside the venv within which the dependencies are ready to execute a programm like e.g. the main.py inside the datageneration.",
            "title": "Local Execution"
        },
        {
            "location": "/analysis/requirements-analysis/",
            "text": "Requirements Analysis for MaLSAMi (Machine Learning based Schedulability Analysis for inter device Migration of software based components during Runtime)\n\u00b6\n\n\nOverview\n\u00b6\n\n\nTasks are running on multiple electronic computing units (ECU). These ECUs periodically create checkpoints of the current state and progress of the running tasks. At some point in time, an ECU might fail. From the checkpoints and the information of the running tasks, the ECU must make a decision for migration of tasks on a failed ECU. This project hopes to utilize schedulability analysis to make better decisions on the migration of these components. After the ECU makes its decision, it must restore the checkpoint on another ECU and have that ECU execute the remaining tasks. \n\n\nThe migration of a task by restoring a checkpoint to another ECU could happen either directly to another ECU or by collecting all the checkpoints of all the ECUs to a different machine and redistributing them to a chosen target ECU when necessary. Another variant of checkpointing would be to create an initial full checkpoint followed by smaller change-based incremental checkpoints. These incremental checkpoints might be integrated into the full checkpoint on the same ECU before sending it to another machine, or every checkpoint is sent right after creation. These smaller based checkpoints are motivated by the necessity of storing and retaining information in the event of an imminent failure. Furthermore, the data would prove to be useful for later analysis. \n\n\nAnother possible use-case for checkpoints, which is not being looked into is the restart of failed tasks at its last known state on the same machine.\n\n\nThe decision of where a task is migrated to is generally based on the information about the task. It could also be a posibillity to include the information gained through the checkpoints to find the best solution.\n\n\nWe have chosen to employ \nmachine learning\n to improve our ability to make these decisions. There are three general phases in which learning would be best applied: \n\n\n\n\nOffline Learning\n Heavyweight phase where there is a lot of data available and enough resources to extensively train the model. We can train on previously sampled data and can utilize our resources for complicated algorithms. This will be done on a different ECU or computer to analyze as much data  as possible and to perform migration planning.\n\n\nOnline Learning\n This type of online learning will be performed on very strong ECUs. Although these ECUs are strong, we do not have access to the entire dataset and all the resources as we are in the offline learning. Furthermore, we will be learning on data in real-time and will not be able to analyze both ends of the dataset. However, this aspect is necessary as it will simulate the migration planning in real time. These decisions will most likely have a greateer influence on the migration planning. \n\n\nonline on normal ECUs (embedded boards)\n This is similiar to the online learning phase described before. The only difference is that the ECU will not be as powerful and may require even lighter weight algorithms. \n\n\n\n\nWhat we have\n\u00b6\n\n\nHardware Available\n\u00b6\n\n\n3 Workstations:\n-   titan V\n\n\n\n\n\n\n3x gtx1080ti\n\n\n\n\n\n\n2x tesla k20c\n    quadro k4200\n\n\n\n\n\n\nData\n\u00b6\n\n\nCheckpointing currently includes only the information in the memory. Checkpointing the capabilities or the registers is not possible yet.\n\n\nNo data yet, we will have to generate it using the implemented distributor.\n\n\nWhich data can be aquired from the distributor is dependent on the defined monitor, but the most information that can be gained are the parameters of each taskset, which were handed over to the genode operating system, and also the start and stop times of each job of each task and also the exit value.\n\n\nAvailable Software\n\u00b6\n\n\n\n\nPython 3.5.2\n\n\nPytorch va\n\n\nCuda v\n\n\nQemu \n\n\nGenode\n\n\ncxxnet\n\n\nTheano\n\n\nTorch7\n\n\n\n\nWhat we can do\n\u00b6\n\n\nMultiple frameworks for deep learning and parallel programming. \n\n\nPytorch allows easy high level implementation of deep neural networks along with GPU accelerated computation. This will be especially useful in accelerating computation of deep neural networks. These GPUs will allow the networks to train on more data in less time. This feature will be especially useful when during the offline training phase where we can train deep and expansive nueral networks with full use of resources.   \n\n\nThere are numerous other frameworks that can also be utilized. Pytorch is good because of its ease in utilizing GPU architecture with neural networks. However, there are numerous other libraries that are specialized to the type of network or learning technique we are using. Cuda-convnet is another project that utilizes C++ Cuda implementatoin of neural networks. If we observe that the neural networks are performing well and we want to optimize, we can then pursue these libraries. \n\n\nShallow Learning techniques are much simpler than Deep Learning techniques and do not always require very sophisticated libraries or hardware. Furthermore, whether or not the learning phase can or should be parallelized can be decided later. Given the resources, we have for the offline training phase, we will attempt to parallelize and optimize the algorithms wherever possible. \n\n\nAfter these rudimentary shallow and deep learning techniques are applied. We can look into reinforcement learning, a newer machine learning approach that is especially used for autonomous driving. The main difference between reinforcement learning and regular supervised/unsupervised machine learning is how an agent decides which actions to take based on the environment. We are currently mostly concerned with schedulablitity analysis, which will be learned based on the data itself. However, further analysis of the acquired data and the patterns analyzed may bring some interest into this type of approach. Other preprocessing and/or unsupervised learning techniques can be implemented to enhance the analysis. This will all depend on the data and the patterns that emerge when it is sampled. The ECUs allow us to use multiple forms of information which will be helpful for machine learning training such as lidar, radars, etc. \n\n\nCheckpointing\n\u00b6\n\n\nCurrent state and snapshot of resources could be used for restarting in case of ECU failure or other problems. Learning can be done by continually monitoring these states and then using them as inputs into the machine learning models. \n\n\nDeep Learning would react better to this as it would be able to properly account for different kinds of input better than shallow learning techniques. Furthermore, the more sophisticated/complicated the data, the more likely that deep learning will perform better.  \n\n\nReinforcement Learning would be helpful in the checkpointing as it will be able to decide whether or not it wants to add a checkpoint based on its environment and available states. Obviously, if the probability of an ecu failing in a particular environment is high, it would be a good idea to add a checkpoint. These are areas that reinforcement learning would be better able to handle rather than regular shallow/deep learning. \n\n\nLearning\n\u00b6\n\n\nWhen it comes to learning techniques, there are many options. MaLSAMi is going to look into Deep and Shallow Learning based data analysis and decision making.\n\n\nDeep Learning\n\u00b6\n\n\nDeep Learning works best when there is a lot of data to sample from. These algorithms will be mostly utilized in the \u2018offline\u2019 phase of the learning. Deep Learning has a variety of different neural network models such as Generative Adversarial Networks (GAN), Spiking Neural Networks (SNN), Feed Forward Networks (FNN), Recurrent Neural Networks (RNN) and Convolutional  CNN.  These networks are specializied for specific types of data mining and analysis and are never a \u2018one fits all\u2019 model. Therefore, we plan to analyze several of these different models. Each of the networks listed below are catered to a certain kind of problem, but they are not too specialized to be unadaptable. \n\n\nShallow Learning\n\u00b6\n\n\nShallow Learning techniques are usually much simpler than deep learning. These require less time and energy to train and classify. However, as they are simpler, they do not have the same adaptibility of the neural networks. However, certain algorithms (Support Vector Machines and Random Forest) have proven to be quite effective in binary classifiaction. The aim of the shallow learning is to do an expansive test of different algorithms and understand what their fitting on the data indicates. High accuracy on certain algorithms will indicate different trends in the data. Regardless of whether or not shallow learning is used, these algorithms will bring more insight into the modelling of the data. \n\n\nTesting\n\u00b6\n\n\nFor both approaches, we will use the standard metrics of testing for shallow and deep learning. This will include shuffling of training splits, cross validation, and in depth classification reports measuring the accuracy, precision, and recall. The challenges with data mining is dealing with improper and unclean data. Optimally, we would have enough data for extensive testing. Hopefully, the distributor will be able to generate enough worthwhile data to train on.",
            "title": "Requirements Analysis"
        },
        {
            "location": "/analysis/requirements-analysis/#requirements-analysis-for-malsami-machine-learning-based-schedulability-analysis-for-inter-device-migration-of-software-based-components-during-runtime",
            "text": "",
            "title": "Requirements Analysis for MaLSAMi (Machine Learning based Schedulability Analysis for inter device Migration of software based components during Runtime)"
        },
        {
            "location": "/analysis/requirements-analysis/#overview",
            "text": "Tasks are running on multiple electronic computing units (ECU). These ECUs periodically create checkpoints of the current state and progress of the running tasks. At some point in time, an ECU might fail. From the checkpoints and the information of the running tasks, the ECU must make a decision for migration of tasks on a failed ECU. This project hopes to utilize schedulability analysis to make better decisions on the migration of these components. After the ECU makes its decision, it must restore the checkpoint on another ECU and have that ECU execute the remaining tasks.   The migration of a task by restoring a checkpoint to another ECU could happen either directly to another ECU or by collecting all the checkpoints of all the ECUs to a different machine and redistributing them to a chosen target ECU when necessary. Another variant of checkpointing would be to create an initial full checkpoint followed by smaller change-based incremental checkpoints. These incremental checkpoints might be integrated into the full checkpoint on the same ECU before sending it to another machine, or every checkpoint is sent right after creation. These smaller based checkpoints are motivated by the necessity of storing and retaining information in the event of an imminent failure. Furthermore, the data would prove to be useful for later analysis.   Another possible use-case for checkpoints, which is not being looked into is the restart of failed tasks at its last known state on the same machine.  The decision of where a task is migrated to is generally based on the information about the task. It could also be a posibillity to include the information gained through the checkpoints to find the best solution.  We have chosen to employ  machine learning  to improve our ability to make these decisions. There are three general phases in which learning would be best applied:    Offline Learning  Heavyweight phase where there is a lot of data available and enough resources to extensively train the model. We can train on previously sampled data and can utilize our resources for complicated algorithms. This will be done on a different ECU or computer to analyze as much data  as possible and to perform migration planning.  Online Learning  This type of online learning will be performed on very strong ECUs. Although these ECUs are strong, we do not have access to the entire dataset and all the resources as we are in the offline learning. Furthermore, we will be learning on data in real-time and will not be able to analyze both ends of the dataset. However, this aspect is necessary as it will simulate the migration planning in real time. These decisions will most likely have a greateer influence on the migration planning.   online on normal ECUs (embedded boards)  This is similiar to the online learning phase described before. The only difference is that the ECU will not be as powerful and may require even lighter weight algorithms.",
            "title": "Overview"
        },
        {
            "location": "/analysis/requirements-analysis/#what-we-have",
            "text": "",
            "title": "What we have"
        },
        {
            "location": "/analysis/requirements-analysis/#hardware-available",
            "text": "3 Workstations:\n-   titan V    3x gtx1080ti    2x tesla k20c\n    quadro k4200",
            "title": "Hardware Available"
        },
        {
            "location": "/analysis/requirements-analysis/#data",
            "text": "Checkpointing currently includes only the information in the memory. Checkpointing the capabilities or the registers is not possible yet.  No data yet, we will have to generate it using the implemented distributor.  Which data can be aquired from the distributor is dependent on the defined monitor, but the most information that can be gained are the parameters of each taskset, which were handed over to the genode operating system, and also the start and stop times of each job of each task and also the exit value.",
            "title": "Data"
        },
        {
            "location": "/analysis/requirements-analysis/#available-software",
            "text": "Python 3.5.2  Pytorch va  Cuda v  Qemu   Genode  cxxnet  Theano  Torch7",
            "title": "Available Software"
        },
        {
            "location": "/analysis/requirements-analysis/#what-we-can-do",
            "text": "Multiple frameworks for deep learning and parallel programming.   Pytorch allows easy high level implementation of deep neural networks along with GPU accelerated computation. This will be especially useful in accelerating computation of deep neural networks. These GPUs will allow the networks to train on more data in less time. This feature will be especially useful when during the offline training phase where we can train deep and expansive nueral networks with full use of resources.     There are numerous other frameworks that can also be utilized. Pytorch is good because of its ease in utilizing GPU architecture with neural networks. However, there are numerous other libraries that are specialized to the type of network or learning technique we are using. Cuda-convnet is another project that utilizes C++ Cuda implementatoin of neural networks. If we observe that the neural networks are performing well and we want to optimize, we can then pursue these libraries.   Shallow Learning techniques are much simpler than Deep Learning techniques and do not always require very sophisticated libraries or hardware. Furthermore, whether or not the learning phase can or should be parallelized can be decided later. Given the resources, we have for the offline training phase, we will attempt to parallelize and optimize the algorithms wherever possible.   After these rudimentary shallow and deep learning techniques are applied. We can look into reinforcement learning, a newer machine learning approach that is especially used for autonomous driving. The main difference between reinforcement learning and regular supervised/unsupervised machine learning is how an agent decides which actions to take based on the environment. We are currently mostly concerned with schedulablitity analysis, which will be learned based on the data itself. However, further analysis of the acquired data and the patterns analyzed may bring some interest into this type of approach. Other preprocessing and/or unsupervised learning techniques can be implemented to enhance the analysis. This will all depend on the data and the patterns that emerge when it is sampled. The ECUs allow us to use multiple forms of information which will be helpful for machine learning training such as lidar, radars, etc.",
            "title": "What we can do"
        },
        {
            "location": "/analysis/requirements-analysis/#checkpointing",
            "text": "Current state and snapshot of resources could be used for restarting in case of ECU failure or other problems. Learning can be done by continually monitoring these states and then using them as inputs into the machine learning models.   Deep Learning would react better to this as it would be able to properly account for different kinds of input better than shallow learning techniques. Furthermore, the more sophisticated/complicated the data, the more likely that deep learning will perform better.    Reinforcement Learning would be helpful in the checkpointing as it will be able to decide whether or not it wants to add a checkpoint based on its environment and available states. Obviously, if the probability of an ecu failing in a particular environment is high, it would be a good idea to add a checkpoint. These are areas that reinforcement learning would be better able to handle rather than regular shallow/deep learning.",
            "title": "Checkpointing"
        },
        {
            "location": "/analysis/requirements-analysis/#learning",
            "text": "When it comes to learning techniques, there are many options. MaLSAMi is going to look into Deep and Shallow Learning based data analysis and decision making.",
            "title": "Learning"
        },
        {
            "location": "/analysis/requirements-analysis/#deep-learning",
            "text": "Deep Learning works best when there is a lot of data to sample from. These algorithms will be mostly utilized in the \u2018offline\u2019 phase of the learning. Deep Learning has a variety of different neural network models such as Generative Adversarial Networks (GAN), Spiking Neural Networks (SNN), Feed Forward Networks (FNN), Recurrent Neural Networks (RNN) and Convolutional  CNN.  These networks are specializied for specific types of data mining and analysis and are never a \u2018one fits all\u2019 model. Therefore, we plan to analyze several of these different models. Each of the networks listed below are catered to a certain kind of problem, but they are not too specialized to be unadaptable.",
            "title": "Deep Learning"
        },
        {
            "location": "/analysis/requirements-analysis/#shallow-learning",
            "text": "Shallow Learning techniques are usually much simpler than deep learning. These require less time and energy to train and classify. However, as they are simpler, they do not have the same adaptibility of the neural networks. However, certain algorithms (Support Vector Machines and Random Forest) have proven to be quite effective in binary classifiaction. The aim of the shallow learning is to do an expansive test of different algorithms and understand what their fitting on the data indicates. High accuracy on certain algorithms will indicate different trends in the data. Regardless of whether or not shallow learning is used, these algorithms will bring more insight into the modelling of the data.",
            "title": "Shallow Learning"
        },
        {
            "location": "/analysis/requirements-analysis/#testing",
            "text": "For both approaches, we will use the standard metrics of testing for shallow and deep learning. This will include shuffling of training splits, cross validation, and in depth classification reports measuring the accuracy, precision, and recall. The challenges with data mining is dealing with improper and unclean data. Optimally, we would have enough data for extensive testing. Hopefully, the distributor will be able to generate enough worthwhile data to train on.",
            "title": "Testing"
        },
        {
            "location": "/analysis/schedulability-analysis/",
            "text": "Schedulability Analysis \n\n\nOn real-time systems, tasks will be executed and processed on multiple different ECUs. These ECUs are responsible for various distributed tasks whose analysis is important for migration planning in the probable event of system failure. Schedulabilit analysis is one of the studies that aims to determine information about how these ECUs are planning tasks. With this information, the behaviors can be modified. \n\n\nOn these multi-ecu systems, the ECUs hold information about the task such as the start/end time. In the event of an ECU failure, it must have a prevention/recovery method in which it can migrate unfinished tasks to other ECUs  that it has not been able to complete. Our goal with schedulability analysis is to garner all the information from past, present, and future events for better damage control and migration planning.\n\n\nCheckpoints are also an important aspect of schedulability analysis. Generally, checkpointing include storing the current state of the machine and/or the task state for postponed execution. These checkpoints could include the memory, the assigned capabilities or the registers. Beside full checkpoints, incremental minor checkpointing could be useful as well. This would include small backups to ensure that no data is lost as a failure of a system can happen at an inconveninet time. \n\n\nThe migration of a task by restoring a checkpoint to another ECU could happen either directly to another ECU or by collecting all the checkpoints of all the ECUs to a different machine and redistributing them to a chosen target ECU when necessary.Another variant of checkpointing would be to create an initial full checkpoint followed by smaller change-based incremental checkpoints. These incremental checkpoints might be integrated into the full checkpoint on the same ECU before sending it to another machine, or every checkpoint is sent right after creation.\n\n\nA possible use-case for checkpoints, which is not being looked into is the restart of failed tasks at its last known state on the same machine. \n\n\nThe decision where a task is migrated to is mainly based on the general information about the task but it could also be a posibillity to include the information gained through the checkpoints to find the best solution.\n\n\nLearning (of any kind) can be performed in three general categories: \n-offline on a different ECU or computer for exactly the purpose of analysing the data and performing the migration planning.\n-online on very strong ECUs which are performing tasks and also the data analysis and decision making necessary for migration\n-online on normal ECUs (embedded boards) which are performing tasks and also the data analysis and decision making necessary for migration\n\n\nThe advantage of offline learning is that we are able to utilize virtually all of our resources and compute time to test multiple algorithms to fit the data. The disadvantage with this is that we will be dealing with a static datasets. Althought this does not imply that the dataset is erroneous, it is not akin to real time data that will happen at the online phase. \n\n\nFurthremore, heavy computation and time used to fit a static datasets very likely leads to overfitting. Even if appropriate measures are made, it is possible that the online data could be different from that of the dataset used. \n\n\nAnalogously, the online training is beneficial and detrimental for the opposite reasons. While online, we do not have as much time and resources as we do while offline. Especially in the situation that we are in now, we must process the data quickly. Therefore, we used the online training as more of a \u2018validation\u2019 stage in which we fine tune the models and hyperparameters. \n\n\nThe distributor is the module responsible for data generation that we will be used for the schedulability analysis. See the \ndistributor file\n for more information about the distributor. \n\n\nTo further analyze the methods we are using, view \nMachine Learning\n and the \nrequirement analysis",
            "title": "Schedulability Analysis"
        },
        {
            "location": "/machine-learning/machine-learning/",
            "text": "Machine Learning\n\u00b6\n\n\nMachine Learning is a useful tool for \npattern matching\n and curve fitting to make predictions based on data and its underlying patterns. Given the recent breakthroughs and ease of use, it is worthwhile to extend the research of schedulability analysis to the lens of machine learning. These algorithms will be able to interpret the data in much more efficient and feasible ways than other algorithms. Since we are using real-time systems, we must also analyze the efficiency and cost of the algorithms and understand the tradeoffs among different techniques. Certain algorithms may be more suitable for the offline training phase while others on the online training phase. All of these are factors which will be considered with this analysis. \n\n\nThe models will be evaluated primarily as a cost benefit analysis with regards to its execution cost (resources required, time to train and classify) and its classification ability. At the very least, machine learning should help to model the data with accuracy where predictions bring more success than standard techniques. \n\n\nWe are pursuing both routes with \nDeep Learning\n and \nShallow_Learning\n to compare the approaches. Although deep learning models are more flexible and advanced than their shallow learning counterparts, it is important to analyze both to see the tradeoffs of both approaches. Since resources and time of execution are more important to consider in the online than the offline learning phases, it is important to see whether we can emulate the accuracy of a heavyweight neural network with a lighter weight shallow-learning algorithm. Furthermore, different algorithms acheive higher accuracy depending on the modelling of the data. A naive bayes classification with high accuracy will indicate data that is mostly independent and uncorrelated with each other.  \n\n\nWe hope that testing a wide variety of these approaches will reveal important information for any kind of future schedulability analysis. Understanding the data is the most important aspect and will provide useful information for any kind of feature engineering that may be done in the future. For more information on the specific algorithms, please see the \nDeep Learning\n and \nShallow_Learning\n pages.",
            "title": "Machine Learning"
        },
        {
            "location": "/machine-learning/machine-learning/#machine-learning",
            "text": "Machine Learning is a useful tool for  pattern matching  and curve fitting to make predictions based on data and its underlying patterns. Given the recent breakthroughs and ease of use, it is worthwhile to extend the research of schedulability analysis to the lens of machine learning. These algorithms will be able to interpret the data in much more efficient and feasible ways than other algorithms. Since we are using real-time systems, we must also analyze the efficiency and cost of the algorithms and understand the tradeoffs among different techniques. Certain algorithms may be more suitable for the offline training phase while others on the online training phase. All of these are factors which will be considered with this analysis.   The models will be evaluated primarily as a cost benefit analysis with regards to its execution cost (resources required, time to train and classify) and its classification ability. At the very least, machine learning should help to model the data with accuracy where predictions bring more success than standard techniques.   We are pursuing both routes with  Deep Learning  and  Shallow_Learning  to compare the approaches. Although deep learning models are more flexible and advanced than their shallow learning counterparts, it is important to analyze both to see the tradeoffs of both approaches. Since resources and time of execution are more important to consider in the online than the offline learning phases, it is important to see whether we can emulate the accuracy of a heavyweight neural network with a lighter weight shallow-learning algorithm. Furthermore, different algorithms acheive higher accuracy depending on the modelling of the data. A naive bayes classification with high accuracy will indicate data that is mostly independent and uncorrelated with each other.    We hope that testing a wide variety of these approaches will reveal important information for any kind of future schedulability analysis. Understanding the data is the most important aspect and will provide useful information for any kind of feature engineering that may be done in the future. For more information on the specific algorithms, please see the  Deep Learning  and  Shallow_Learning  pages.",
            "title": "Machine Learning"
        },
        {
            "location": "/machine-learning/shallow-learning/shallow-learning/",
            "text": "Shallow Learning\n\u00b6\n\n\nShallow Learning represents all the machine learning algorithms the techniques that are not \u2018deep learning\u2019 or in the case of this project, those of which do not utilize a layered-neural network or multi-layer perceptron. Although virtually any technique can be tested under the guise of \u2018shallow learning\u2019, we chose to focus on k-Nearest Neibhbors (kNN), Logistic Regression, Gaussian Naive Bayes, and Decision Trees/Random Forests (Support Vector Machine functionality is present, but the training was too long to extract meaningful results). The success of these algorithms is almost entirely dependent on the patterns that are present in the data. These algorithms have different strengths, and we are hoping to not only analyze how well these algorithms can perform, but also how they can be applied to embedded systems or online-machine learning. \n\n\nTesting Setup\n\u00b6\n\n\nThe shallow learning models were taken from the Sci-kit learn python class. The current machine learning pipeline allows the user to develop his or her own shallow learning models if he or she does not wish to use those of sci-kit learn. The Machine Learning Pipeline currently wraps the sci-kit learn machine learning models and some of the important methods. \n\n\nPopular evaluation metrics on data include data splitting (train, validation, testing sets), k-fold cross-validation, and parameterized grid search for optimal hyperparameter search. \n\n\nGiven the extraordinary amount of time that grid search takes, a \u2018randomized\u2019 search was used instead. Both of these searches take a list of parameters specified by the user and cross checks them to obtain the best parameters. While a grid search will test every parameter value, the ranomized search searches only a small fraction of them. This is the default search that we used. The developer is welcome to revert to the exhaustive Grid search by toggling the parameter. However, the results presented below will show only the results done by a \u2018randomized\u2019 grid search and not an exhaustive one. \n\n\nAlso, the parameters chosen were completely arbitrary. A developer may get different results not only from different hyperparameter selection, but also from different grid search runs. The grid search is only used to give a basic idea of what is going on. \n\n\nLogistic Regression/Classification\n\u00b6\n\n\nLogistic Regression will determine attempt to model any relationship between the measured data and the label data. In the case of taskset data, logistic regression will attemp to model a relationship between the various tasksets. \n\n\nIf you are familiar with neural networks, logistic regression is simply a neural network without the hidden layer. It will give weights to the data point(s) based on the features.  \n\n\nRandomized Grid Search Results on pandas data:\n\n\nBest Penalty Type\n: \nL1\n\n\nBest C (Regularization Strength)\n: \n1291.5496650148827\n\n\n(Rest are default parameters)\n\n\nNaive Bayes Algorithm\n\u00b6\n\n\nNaive Bayes algorithm is \u2018naive\u2019 because it implicity assumes independence among all individual training examples. As we know with data systems, let alone taskset data, it is very unlikely that the data is independent. However, if we were to get a good performance with this classificaton algorithm, it would inidcate that the data is uncorrelated. \n\n\nAlthough more sophisticated algorithms exist, naive bayes is very good for efficient computation. Because of its assumption of independence, the order of the input or the arrangement of the data does not affect the overall training experience. This is especially important in online-learning when all the data is not immediately available to us. \n\n\nK-Nearest Neighbors\n\u00b6\n\n\nThe K-Nearest Neighbors is one of the simples algorithm that uses rudimentary techniques to classify data. It is a flexible approach and can be useful for determining locality information. This information could further be used for unsupervised learning techniques such as clustering and dimension reduction. Unfortunately, this method does not scale well with multi-feature data similiar to the data that we currently have. However, this algorithm is fairly simple to implement and test. It will serve as a meaningful control algorithm (one in which we compare our fancier algorithms to). \n\n\nRandomized Grid Search Results on pandas data:\n\n\nBest Number of K-Neighbors\n: \n5\n\n\n(Rest are default parameters)\n\n\nRandom Forests/Decision Trees\n\u00b6\n\n\nDecision Trees are great for handling categorical information without having to do much preprocessing. After the training phase, it is able to classify data quickly. This will be especially useful in the online learning phase. The main issue with decision trees is its likelihood of overfitting. This is where random forests come in to the picture.  \n\n\nRandom Forests are a very powerful algorithm and handle the low variance issues of decision trees. Although they are a little more difficult to implement/tune, they are expected to be one of the more useful algorithms. Much like decision trees, they can classify test data very quickly.   \n\n\nRandomized Grid Search Results on pandas data:\n\n\nBest Depth\n: \n70\n\n\nBest Max Features\n: \nauto\n\n\nBest min_samples_leaf\n: \n3\n\n\nBest min_samples_split\n: \n4\n\n\nBest n_estimators\n: \n1000\n\n\n(Rest are default sci-kit learn parameters)\n\n\nSuppor Vector Machines (svm)\n\u00b6\n\n\nSuppor Vector machines are effective classifiers for any kind of classification task. A linear support vector machine  may not separate the data as well as a non-linear svm. However, for completeness, we will train several different models on the data to obtain the best fit. Although this is an exhaustive process, support vector machines are relatively lightweight and very highly regarded for binary classification tasks. Additionally, the preproccesing step to divide the classes into respective classification assignments is also important and will assist in more accuracte predictions. \n\n\nWe fitted a support vector machine with a polynomial kernel. For completeness, we added functionality for grid search (GridSearchCV) which fits the support vector machine with multiple different hyperparameters. (Note about the grid search, it is an exhaustive execution that will take a lot of time and energy. We recommend enabling GPU support, however it is unclear whether or not performance will ampify as much as it would for deep learning. Do not be surprised by long computation time. A quicker alternative would be to manually tune the hyperparameters with a training and validation set). \n\n\nIn general, the Support Vector Machines with the Gaussian kernels perform optimally.",
            "title": "Shallow learning"
        },
        {
            "location": "/machine-learning/shallow-learning/shallow-learning/#shallow-learning",
            "text": "Shallow Learning represents all the machine learning algorithms the techniques that are not \u2018deep learning\u2019 or in the case of this project, those of which do not utilize a layered-neural network or multi-layer perceptron. Although virtually any technique can be tested under the guise of \u2018shallow learning\u2019, we chose to focus on k-Nearest Neibhbors (kNN), Logistic Regression, Gaussian Naive Bayes, and Decision Trees/Random Forests (Support Vector Machine functionality is present, but the training was too long to extract meaningful results). The success of these algorithms is almost entirely dependent on the patterns that are present in the data. These algorithms have different strengths, and we are hoping to not only analyze how well these algorithms can perform, but also how they can be applied to embedded systems or online-machine learning.",
            "title": "Shallow Learning"
        },
        {
            "location": "/machine-learning/shallow-learning/shallow-learning/#testing-setup",
            "text": "The shallow learning models were taken from the Sci-kit learn python class. The current machine learning pipeline allows the user to develop his or her own shallow learning models if he or she does not wish to use those of sci-kit learn. The Machine Learning Pipeline currently wraps the sci-kit learn machine learning models and some of the important methods.   Popular evaluation metrics on data include data splitting (train, validation, testing sets), k-fold cross-validation, and parameterized grid search for optimal hyperparameter search.   Given the extraordinary amount of time that grid search takes, a \u2018randomized\u2019 search was used instead. Both of these searches take a list of parameters specified by the user and cross checks them to obtain the best parameters. While a grid search will test every parameter value, the ranomized search searches only a small fraction of them. This is the default search that we used. The developer is welcome to revert to the exhaustive Grid search by toggling the parameter. However, the results presented below will show only the results done by a \u2018randomized\u2019 grid search and not an exhaustive one.   Also, the parameters chosen were completely arbitrary. A developer may get different results not only from different hyperparameter selection, but also from different grid search runs. The grid search is only used to give a basic idea of what is going on.",
            "title": "Testing Setup"
        },
        {
            "location": "/machine-learning/shallow-learning/shallow-learning/#logistic-regressionclassification",
            "text": "Logistic Regression will determine attempt to model any relationship between the measured data and the label data. In the case of taskset data, logistic regression will attemp to model a relationship between the various tasksets.   If you are familiar with neural networks, logistic regression is simply a neural network without the hidden layer. It will give weights to the data point(s) based on the features.    Randomized Grid Search Results on pandas data:  Best Penalty Type :  L1  Best C (Regularization Strength) :  1291.5496650148827  (Rest are default parameters)",
            "title": "Logistic Regression/Classification"
        },
        {
            "location": "/machine-learning/shallow-learning/shallow-learning/#naive-bayes-algorithm",
            "text": "Naive Bayes algorithm is \u2018naive\u2019 because it implicity assumes independence among all individual training examples. As we know with data systems, let alone taskset data, it is very unlikely that the data is independent. However, if we were to get a good performance with this classificaton algorithm, it would inidcate that the data is uncorrelated.   Although more sophisticated algorithms exist, naive bayes is very good for efficient computation. Because of its assumption of independence, the order of the input or the arrangement of the data does not affect the overall training experience. This is especially important in online-learning when all the data is not immediately available to us.",
            "title": "Naive Bayes Algorithm"
        },
        {
            "location": "/machine-learning/shallow-learning/shallow-learning/#k-nearest-neighbors",
            "text": "The K-Nearest Neighbors is one of the simples algorithm that uses rudimentary techniques to classify data. It is a flexible approach and can be useful for determining locality information. This information could further be used for unsupervised learning techniques such as clustering and dimension reduction. Unfortunately, this method does not scale well with multi-feature data similiar to the data that we currently have. However, this algorithm is fairly simple to implement and test. It will serve as a meaningful control algorithm (one in which we compare our fancier algorithms to).   Randomized Grid Search Results on pandas data:  Best Number of K-Neighbors :  5  (Rest are default parameters)",
            "title": "K-Nearest Neighbors"
        },
        {
            "location": "/machine-learning/shallow-learning/shallow-learning/#random-forestsdecision-trees",
            "text": "Decision Trees are great for handling categorical information without having to do much preprocessing. After the training phase, it is able to classify data quickly. This will be especially useful in the online learning phase. The main issue with decision trees is its likelihood of overfitting. This is where random forests come in to the picture.    Random Forests are a very powerful algorithm and handle the low variance issues of decision trees. Although they are a little more difficult to implement/tune, they are expected to be one of the more useful algorithms. Much like decision trees, they can classify test data very quickly.     Randomized Grid Search Results on pandas data:  Best Depth :  70  Best Max Features :  auto  Best min_samples_leaf :  3  Best min_samples_split :  4  Best n_estimators :  1000  (Rest are default sci-kit learn parameters)",
            "title": "Random Forests/Decision Trees"
        },
        {
            "location": "/machine-learning/shallow-learning/shallow-learning/#suppor-vector-machines-svm",
            "text": "Suppor Vector machines are effective classifiers for any kind of classification task. A linear support vector machine  may not separate the data as well as a non-linear svm. However, for completeness, we will train several different models on the data to obtain the best fit. Although this is an exhaustive process, support vector machines are relatively lightweight and very highly regarded for binary classification tasks. Additionally, the preproccesing step to divide the classes into respective classification assignments is also important and will assist in more accuracte predictions.   We fitted a support vector machine with a polynomial kernel. For completeness, we added functionality for grid search (GridSearchCV) which fits the support vector machine with multiple different hyperparameters. (Note about the grid search, it is an exhaustive execution that will take a lot of time and energy. We recommend enabling GPU support, however it is unclear whether or not performance will ampify as much as it would for deep learning. Do not be surprised by long computation time. A quicker alternative would be to manually tune the hyperparameters with a training and validation set).   In general, the Support Vector Machines with the Gaussian kernels perform optimally.",
            "title": "Suppor Vector Machines (svm)"
        },
        {
            "location": "/machine-learning/deep-learning/",
            "text": "Deep Learning\n\u00b6\n\n\nThe use of deep learning has pervaded aritificial intelligence and is being applied to virtually all projects. Deep Learning\u2019s strengths lie in its ability to pattern match data with complicated structures given that it has enough training data. Although deep learning seems to be the obvious answer with its advanced techniques, it is relatively heavyweight. Deep Learning works best when there is a lot of data to sample from and if possible, if we have an idea of what the data is like.  \n\n\nThere are virtually endless number of deep learning models that are possible to try out. We have listed a few of them below. \n\n\nFeed Forward Neural Network\n\u00b6\n\n\nA Feed-forward artificial neural network is a multilayer perceptron and is the most basic neural network available. A feed forward network is a necessary model and will act as the \ncontrol\n network for the rest of the deep learning models. Feed Forward Networks are adaptable in fitting simple data. We hope to acheive as high of a possible of an accuracy on this type of network before venturing into the more advanced networks listed below. A simple Feed Forward Net has already been trained and fitted with some success (80% accuracy.)\n\n\nAn implementation for a basic Feed Forward Network already exists. The repo and expansive documentation is provided \nhere\n. \n\n\nRecurrent Neural Networks\n\u00b6\n\n\nRecurrent Neural Networks are types of networks which utilizes \u2018memory\u2019. These networks are very similiar to regular feed forward networks except for their ability to process previously analyzed along with the data that is currently being processed. These types of networks are especially beneficial for temporal and sequence analysis. This type of network could prove to be fruitful in the online learning phase as this network can better analyze recent events and use them in the learning process. At the same time, thie network will still be heavy duty and possibly computationally more expensive than the regular feed-forward network. \n\n\nGenerative Adversarial Networks\n\u00b6\n\n\nA GAN generally does not describe the structure of the neural network, but rather two types of neural networks which work together (or against each other). One generates data similar to some real dataset and the second network, which was formerly trained on the original dataset evaluates the generated data. The generator intends on \u201cfooling\u201d the evaluating network into classifying the generated data as real. While training, the generator becomes better at creating data close to the real dataset and the evaluating network becomes better at flagging faulty or erroneous data. \n\n\nWe can think of the generator as producing the data and the discriminator as a regular supervised network that is no different from the others. The discriminator however can classify real and fake data more accurately. The generator produces samples to \u2018misclassify\u2019 the discriminator while the discriminator is training to counteract it.  \n\n\nConvolutional Neural Networks\n\u00b6\n\n\nA convolutional networks biggest strength is in its ability to extract the relevant features from an extremely large dataset. Convolutional networks are usually used in image processing as they are effective at processing images based on the information different areas of the image gives. Other than image processing, these types of networks are very good at measuring structured information. This includes text classificatoin and other problems in which the data\u2019s placement gives clues as to its meaning. If the data from the distributor happens to be structured in any way, the convolutional network has a good chance of performing well on it. \n\n\nSpiking Neural Networks\n\u00b6\n\n\nSpiking Neural Networks refer to any network in which the input nodes(neurons) propogate the information at different times throughout the network. Each input neuron has an activation level in which incoming spikes determines pushes the function higher or lower. These networks most closely model real neurons in the brain, as all potential information is not fully processed at each time. The problem is that spiking is a noisy process, and may skew the data. The spikes are part of the learning process, and so the times the neurons are activated is analyzed just as much as what the neurons are sending. The idea of spiking can be applied to any of the other neural networks above, as it resembles more of a hyperparameter than an actual unique network model. A network using spiking is difficult to train as the signal nature of the spikes may be non-continuous and non-differentiable.",
            "title": "Deep Learning"
        },
        {
            "location": "/machine-learning/deep-learning/#deep-learning",
            "text": "The use of deep learning has pervaded aritificial intelligence and is being applied to virtually all projects. Deep Learning\u2019s strengths lie in its ability to pattern match data with complicated structures given that it has enough training data. Although deep learning seems to be the obvious answer with its advanced techniques, it is relatively heavyweight. Deep Learning works best when there is a lot of data to sample from and if possible, if we have an idea of what the data is like.    There are virtually endless number of deep learning models that are possible to try out. We have listed a few of them below.",
            "title": "Deep Learning"
        },
        {
            "location": "/machine-learning/deep-learning/#feed-forward-neural-network",
            "text": "A Feed-forward artificial neural network is a multilayer perceptron and is the most basic neural network available. A feed forward network is a necessary model and will act as the  control  network for the rest of the deep learning models. Feed Forward Networks are adaptable in fitting simple data. We hope to acheive as high of a possible of an accuracy on this type of network before venturing into the more advanced networks listed below. A simple Feed Forward Net has already been trained and fitted with some success (80% accuracy.)  An implementation for a basic Feed Forward Network already exists. The repo and expansive documentation is provided  here .",
            "title": "Feed Forward Neural Network"
        },
        {
            "location": "/machine-learning/deep-learning/#recurrent-neural-networks",
            "text": "Recurrent Neural Networks are types of networks which utilizes \u2018memory\u2019. These networks are very similiar to regular feed forward networks except for their ability to process previously analyzed along with the data that is currently being processed. These types of networks are especially beneficial for temporal and sequence analysis. This type of network could prove to be fruitful in the online learning phase as this network can better analyze recent events and use them in the learning process. At the same time, thie network will still be heavy duty and possibly computationally more expensive than the regular feed-forward network.",
            "title": "Recurrent Neural Networks"
        },
        {
            "location": "/machine-learning/deep-learning/#generative-adversarial-networks",
            "text": "A GAN generally does not describe the structure of the neural network, but rather two types of neural networks which work together (or against each other). One generates data similar to some real dataset and the second network, which was formerly trained on the original dataset evaluates the generated data. The generator intends on \u201cfooling\u201d the evaluating network into classifying the generated data as real. While training, the generator becomes better at creating data close to the real dataset and the evaluating network becomes better at flagging faulty or erroneous data.   We can think of the generator as producing the data and the discriminator as a regular supervised network that is no different from the others. The discriminator however can classify real and fake data more accurately. The generator produces samples to \u2018misclassify\u2019 the discriminator while the discriminator is training to counteract it.",
            "title": "Generative Adversarial Networks"
        },
        {
            "location": "/machine-learning/deep-learning/#convolutional-neural-networks",
            "text": "A convolutional networks biggest strength is in its ability to extract the relevant features from an extremely large dataset. Convolutional networks are usually used in image processing as they are effective at processing images based on the information different areas of the image gives. Other than image processing, these types of networks are very good at measuring structured information. This includes text classificatoin and other problems in which the data\u2019s placement gives clues as to its meaning. If the data from the distributor happens to be structured in any way, the convolutional network has a good chance of performing well on it.",
            "title": "Convolutional Neural Networks"
        },
        {
            "location": "/machine-learning/deep-learning/#spiking-neural-networks",
            "text": "Spiking Neural Networks refer to any network in which the input nodes(neurons) propogate the information at different times throughout the network. Each input neuron has an activation level in which incoming spikes determines pushes the function higher or lower. These networks most closely model real neurons in the brain, as all potential information is not fully processed at each time. The problem is that spiking is a noisy process, and may skew the data. The spikes are part of the learning process, and so the times the neurons are activated is analyzed just as much as what the neurons are sending. The idea of spiking can be applied to any of the other neural networks above, as it resembles more of a hyperparameter than an actual unique network model. A network using spiking is difficult to train as the signal nature of the spikes may be non-continuous and non-differentiable.",
            "title": "Spiking Neural Networks"
        },
        {
            "location": "/machine-learning/nets/ffn/",
            "text": "Feed Forward Network Documentation\n\u00b6\n\n\nThis project attempted to train a feed forward network on taskset parameters to predict whether or not a taskset represents an executable combination of tasks.\n\n\nIn the end it is a four layers feed forward neural network with two hidden layers of size 50. The hidden layers each had leaky-RELU non-linearities. \n\n\nAlthough ReLU functions are very popular with most Feed-Forward Neural Networks. ReLU functions are popular because of their handling of the vanishing gradient that often arises from other activation functions. The advantage of Leaky ReLU is that it preserves more of the data than a regular ReLU function. While the ReLU chooses the max(0,input), the leaky-ReLU will choose: x for x \u2265 0 and otherwise \u03b1 * x where \u03b1 is an argument to the function, with a default value of 0.01.\n\n\nThe input dimension (N X D) constituted the number of tasksets (N) where each taskset contained a certain number of tasks. The parameters of these individual tasks within the taskset made up the parameters/features of the taskset itself. In our case D equals 12 for a taskset of size three. \n\n\nThe output dimension (N X 2) is two neurons which each output a value between \u20181\u2019 and \u20180\u2019 using a sigmoid function. \n\n\nThe final configuration of the network consists of the following:\n\n\n\n\nmini batch size: 30\n\n\nhidden layers: 2, both of size 50\n\n\nlearning rate: 1e-5\n\n\nloss function: CrossEntropyLoss\n\n\noptimizer: Adam\n\n\n\n\nThe preprocessing of the data included the removal of parameters that were not unique, thus having no impact on the training, removing some tasksets, which have parameters that were thought to be outliers that distorted the normalized data followed, of course by normalizing the input values.\nAfter about 200 epochs improvement starts to stagnate at an accuracy of about 70% if an error of up to 0.1 is allowed.\nThe next 950 epochs only show an improvement of another 4% on the same data.\n\n\nThe available data was split into 60% training, 20% validation and 20% test data.\nOut of the 13 parameters per task only 4 were distinct. In the end only the priority, the argument, the period and the number of jobs were kept.\n\n\nPreviously to the work in this project a Bachelors Thesis tried to train a small feed forward network on a very limited dataset. The thesis only used a tasktype identifyer and the given critical time as inputs. The code is available \nhere\n.",
            "title": "Feed Forward Network"
        },
        {
            "location": "/machine-learning/nets/ffn/#feed-forward-network-documentation",
            "text": "This project attempted to train a feed forward network on taskset parameters to predict whether or not a taskset represents an executable combination of tasks.  In the end it is a four layers feed forward neural network with two hidden layers of size 50. The hidden layers each had leaky-RELU non-linearities.   Although ReLU functions are very popular with most Feed-Forward Neural Networks. ReLU functions are popular because of their handling of the vanishing gradient that often arises from other activation functions. The advantage of Leaky ReLU is that it preserves more of the data than a regular ReLU function. While the ReLU chooses the max(0,input), the leaky-ReLU will choose: x for x \u2265 0 and otherwise \u03b1 * x where \u03b1 is an argument to the function, with a default value of 0.01.  The input dimension (N X D) constituted the number of tasksets (N) where each taskset contained a certain number of tasks. The parameters of these individual tasks within the taskset made up the parameters/features of the taskset itself. In our case D equals 12 for a taskset of size three.   The output dimension (N X 2) is two neurons which each output a value between \u20181\u2019 and \u20180\u2019 using a sigmoid function.   The final configuration of the network consists of the following:   mini batch size: 30  hidden layers: 2, both of size 50  learning rate: 1e-5  loss function: CrossEntropyLoss  optimizer: Adam   The preprocessing of the data included the removal of parameters that were not unique, thus having no impact on the training, removing some tasksets, which have parameters that were thought to be outliers that distorted the normalized data followed, of course by normalizing the input values.\nAfter about 200 epochs improvement starts to stagnate at an accuracy of about 70% if an error of up to 0.1 is allowed.\nThe next 950 epochs only show an improvement of another 4% on the same data.  The available data was split into 60% training, 20% validation and 20% test data.\nOut of the 13 parameters per task only 4 were distinct. In the end only the priority, the argument, the period and the number of jobs were kept.  Previously to the work in this project a Bachelors Thesis tried to train a small feed forward network on a very limited dataset. The thesis only used a tasktype identifyer and the given critical time as inputs. The code is available  here .",
            "title": "Feed Forward Network Documentation"
        },
        {
            "location": "/genode/genode/",
            "text": "Genode & Fiasco Update\n\u00b6\n\n\nThis section covers the updates for Genode and Fiasco.OC. \n\n\nAgenda\n\u00b6\n\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0+\n\n\n\n\n\n +\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\u00b6\n\n\nThe default setup listed below is based on Genode 16.08 and foc r67.\n\nTo use the new versions Genode 18.02 and foc r78 please take a look at \u201cStructure of directories\u201d.\n\n\nIf you like to use a VM for this project,\n\nplease follow the steps on the \nArgOS-research website\n.\n\n\nIf you like to use your native machine,\n\njust clone https://github.com/argos-research/operating-system.git branch master\n\nand execute ./setup.sh .\n\nThis will build the project \u201cdom0-HW\u201d on platform \u201cfocnados_panda\u201d by default.\n\nPlease adjust the MAKEFILE to your needs.\n\n\nStructure of directories\n\u00b6\n\n\nGenode 18.02 and foc r78:\n\nhttps://github.com/malsami/genode\n\nbranch 18.02_r78\n\n\nGenode 18.02 and focnados r78:\n\nhttps://github.com/malsami/genode\n\nbranch focnados_18.02_r78\n\n\nCheckpoint Restore and Taskloader for Genode 18.02 and focnados r78:\n\nhttps://github.com:argos-research/operating-system.git\n\nbranch 18.02",
            "title": "Genode"
        },
        {
            "location": "/genode/genode/#genode-fiasco-update",
            "text": "This section covers the updates for Genode and Fiasco.OC.",
            "title": "Genode &amp; Fiasco Update"
        },
        {
            "location": "/genode/genode/#agenda",
            "text": "+    +",
            "title": "Agenda"
        },
        {
            "location": "/genode/genode/#setup",
            "text": "The default setup listed below is based on Genode 16.08 and foc r67. \nTo use the new versions Genode 18.02 and foc r78 please take a look at \u201cStructure of directories\u201d.  If you like to use a VM for this project, \nplease follow the steps on the  ArgOS-research website .  If you like to use your native machine, \njust clone https://github.com/argos-research/operating-system.git branch master \nand execute ./setup.sh . \nThis will build the project \u201cdom0-HW\u201d on platform \u201cfocnados_panda\u201d by default. \nPlease adjust the MAKEFILE to your needs.",
            "title": "Setup"
        },
        {
            "location": "/genode/genode/#structure-of-directories",
            "text": "Genode 18.02 and foc r78: \nhttps://github.com/malsami/genode \nbranch 18.02_r78  Genode 18.02 and focnados r78: \nhttps://github.com/malsami/genode \nbranch focnados_18.02_r78  Checkpoint Restore and Taskloader for Genode 18.02 and focnados r78: \nhttps://github.com:argos-research/operating-system.git \nbranch 18.02",
            "title": "Structure of directories"
        },
        {
            "location": "/data-generation/distributor/",
            "text": "Distributor\n\u00b6\n\n\nDistributor is the main class/entity that controls and distributes host sessions to manage tasksets. Upon startup, the distributor will have a preset value of one host session that it will be able to spawn. The user is permitted to change this number at any time up to a hardcoded limit of fortytwo (QEMU instances). Machines set to be closed will finish their assigned taskset before being shut down. The distributor is mainly used for gathering large-scale data about autonomous system schedulability-tasks, which can be used for data mining and analysis. \n\n\nSet up and Configuration\n\u00b6\n\n\nClone the client-tools repository, move to the client-tools directory and execute the make commands as shown below. The value \u201cfocnados_pbxa9\u201d that is passed to the OS-TARGET argument can be substituted for the intended target.\n\n\ngit clone https://www.github.com/malsami/client-tools\ncd client-tools\nmake distributor-init\nmake make genode-init\nmake binaries OS-TARGET=focnados_pbxa9\n\n\n\n\nand initialize the submodules and place a working \nimage.elf\n file inside the client-tools directory.\n\n\nWith Vagrant\n\u00b6\n\n\nA \nvagrant\n script is provided in the client-tools repository, which will set up the appropriate development environment and prepare it for use without any manual installation or configuration.\nThis can be setup via\n\n\nmake vagrant\n\n\n\n\nafter telling vagrant to wich interface to connect, you will end up inside a vagrant machine.\nIf you \n\n\ncd /Vagrant\n\n\n\n\nyou will be in the mounted client-tools folder.\n\n\n(In the past, executing the distributor inside a vagrant machine lead to an unknown error, which stops the execution at some point after about ten to thirty hours. As the vagrant machine serves only as a test and development environment, this is not a critical issue and will not be further investigated, as mentioned in the according \nissue\n. Whether this is still the case or not is unclear, as no distributor ran that long inside a vagrant machine ever again.)\n\n\nManual Setup\n\u00b6\n\n\nTo run the distrubutor directly on your system, the bridge functionality and networking components have to be installed.\n\n\nInstall bridge functionality for networking:\n\n\nsudo apt-get install bridge-utils -qq\n\n\n\nDownload DHCP service and move provided configuration file. \n\n\nsudo apt-get install isc-dhcp-server -qq\nsudo cp dhcpd.conf /etc/dhcp/\n\n\n\nSet up the bridge and assign an IP address (consult the dhcp.conf file for more information on IP ranges)\n\n\nsudo brctl addbr br0\nsudo ip addr add 10.200.45.254/24 dev br0\n\n\n\nAdjust /etc/network/interfaces file with preliminary networking information\n\n\nsudo sh -c 'echo \"\\nauto br0\\niface br0 inet static\\n\\thwaddress ether DE:AD:BE:EF:69:01\\n\\taddress 10.200.45.254\\n\\tnetmask 255.255.255.0\\n\\tgateway 10.200.45.254\\nbridge_ports eth0\\nbridge_stp off\\nbridge_maxwait 0\\nbridge_fd 0\\n\" >> /etc/network/interfaces'\n\n\n\nInstall qemu and screen for spawning host sessions from the distributor and for easy visualization of spawned sessions. \n\n\nmake qemu\nsudo apt-get install screen -qq\n\n\n\nStart the dhcp service\n\n\nsudo systemctl start isc-dhcp-server\n\n\n\nThe machine should now be ready for use. \n\n\nUsing the Distributor\n\u00b6\n\n\nThe following is an example execution to provide better understanding.\nFirst change into the created python environment by typing\n\n\nsource malsami/bin/activate\n\n\n\nYou are now inside the python environment. (To leave it just type \u201cdeactivate\u201d)\n\n\nOpen up the interactive python shell\n in the directory \u2018distributor\u2019 by typing (You can also create and execute this in a script)\n\n\nsudo python3\n\n\n\nsudo is necessary to allow for the creation and destruction of tap devices, via which the qemu instances are connected to the bridge.\n\n\nAdd appropriate imports:\n \n\n\n(\u2018example.py\u2019 holds some tasksets for testing, \u2018loggingMonitor.py\u2019 is a \nMonitor\n for testing, which writes to logs/monitor.log)\n\n\nfrom test import example5\nfrom distributor.distributorClass import Distributor\nfrom distributor.monitors.loggingMonitor import LoggingMonitor\n\n\n\nDefining a monitor and tasksets for the execution is left to the user.\n\n\nInitialize modules:\n\n\nt = example5()\nlm = LoggingMonitor()\ndist = Distributor()\n\n\n\nNow the distributor is running.\n\n\nAdding a job\n is possible via the add_jobs([taskset], monitor) function: \n\n\ndist.add_job([t],lm)\n\n\n\nThis will spawn machines acording to the current max_machine value.\n\n\nNote: You can repeat the above command to queue multiple jobs whenever you please.\n\n\nTo view a list of detached qemu instances \n\n\nsudo screen -ls\n\n\n\nTo kill a detached screen type:\n\n\nsudo screen -X -S <name\\_of\\_screen> kill\n\n\n\nThe log files of the genode instances are also saved in the distributor/log/ directory.\n\n\nAdditionally, you can adjust the number of spawned machines, also while the distributor is running. See the distributor functions for more information. \n\n\nDistributor functions\n\u00b6\n\n\nSetting max machines to a value between 1 and 42. You can change this anytime as this only affects the maximum total number of spawned machines. If machines are active, the number will be adapted accordingly. Closing machines will still finish their current taskset before shutting down. (The argument \u201cmax_allowed\u201d can be passed in the Distributors init() to adjust the limit.):\n\n\n    set_max_machine_value(numMachines)\n\n\n\nFunction to check if the distributor is busy or not:\n\n\n    get_distributor_state()\n\n\n\nReturn current maximum value of possible active machines:\n\n\n    get_max_machine_value()\n\n\n\nCreating a new job and adding it to a list of jobs to be worked on.\nThe function is instantiating a new object of type _Job which is then appended to the list of jobs to be processed:\n\n\n    add_job(tasksetList, monitor=None, offset=0, *session_params)\n\n\n\nA job always consists of list of TaskSet tasksetList and a Monitor implementing AbstractMonitor from monitor.py, the session_parameters are optional. Inside the method a _Job() will be created to hold the iterator over the list.\n\n\nKill all machines that are currently running \nwithout\n waiting for current tasksets to finish:\n\n\n    kill_all_machines()\n\n\n\nStop all machines after they finished the execution of their current taskset:\n\n\n    shut_down_all_machines()\n\n\n\nResume stopped machines:\n\n\n    resume()\n\n\n\nThe Machine class\n\u00b6\n\n\nThe \u2018machine.py\u2019 implements a class which extends threading.Thread.\n\n\nAn instance of Machine is taking care of spawning a host, creating a session which connects to the spawned host and acquiring tasksets while there still is work to be done.",
            "title": "Distributor"
        },
        {
            "location": "/data-generation/distributor/#distributor",
            "text": "Distributor is the main class/entity that controls and distributes host sessions to manage tasksets. Upon startup, the distributor will have a preset value of one host session that it will be able to spawn. The user is permitted to change this number at any time up to a hardcoded limit of fortytwo (QEMU instances). Machines set to be closed will finish their assigned taskset before being shut down. The distributor is mainly used for gathering large-scale data about autonomous system schedulability-tasks, which can be used for data mining and analysis.",
            "title": "Distributor"
        },
        {
            "location": "/data-generation/distributor/#set-up-and-configuration",
            "text": "Clone the client-tools repository, move to the client-tools directory and execute the make commands as shown below. The value \u201cfocnados_pbxa9\u201d that is passed to the OS-TARGET argument can be substituted for the intended target.  git clone https://www.github.com/malsami/client-tools\ncd client-tools\nmake distributor-init\nmake make genode-init\nmake binaries OS-TARGET=focnados_pbxa9  and initialize the submodules and place a working  image.elf  file inside the client-tools directory.",
            "title": "Set up and Configuration"
        },
        {
            "location": "/data-generation/distributor/#with-vagrant",
            "text": "A  vagrant  script is provided in the client-tools repository, which will set up the appropriate development environment and prepare it for use without any manual installation or configuration.\nThis can be setup via  make vagrant  after telling vagrant to wich interface to connect, you will end up inside a vagrant machine.\nIf you   cd /Vagrant  you will be in the mounted client-tools folder.  (In the past, executing the distributor inside a vagrant machine lead to an unknown error, which stops the execution at some point after about ten to thirty hours. As the vagrant machine serves only as a test and development environment, this is not a critical issue and will not be further investigated, as mentioned in the according  issue . Whether this is still the case or not is unclear, as no distributor ran that long inside a vagrant machine ever again.)",
            "title": "With Vagrant"
        },
        {
            "location": "/data-generation/distributor/#manual-setup",
            "text": "To run the distrubutor directly on your system, the bridge functionality and networking components have to be installed.  Install bridge functionality for networking:  sudo apt-get install bridge-utils -qq  Download DHCP service and move provided configuration file.   sudo apt-get install isc-dhcp-server -qq\nsudo cp dhcpd.conf /etc/dhcp/  Set up the bridge and assign an IP address (consult the dhcp.conf file for more information on IP ranges)  sudo brctl addbr br0\nsudo ip addr add 10.200.45.254/24 dev br0  Adjust /etc/network/interfaces file with preliminary networking information  sudo sh -c 'echo \"\\nauto br0\\niface br0 inet static\\n\\thwaddress ether DE:AD:BE:EF:69:01\\n\\taddress 10.200.45.254\\n\\tnetmask 255.255.255.0\\n\\tgateway 10.200.45.254\\nbridge_ports eth0\\nbridge_stp off\\nbridge_maxwait 0\\nbridge_fd 0\\n\" >> /etc/network/interfaces'  Install qemu and screen for spawning host sessions from the distributor and for easy visualization of spawned sessions.   make qemu\nsudo apt-get install screen -qq  Start the dhcp service  sudo systemctl start isc-dhcp-server  The machine should now be ready for use.",
            "title": "Manual Setup"
        },
        {
            "location": "/data-generation/distributor/#using-the-distributor",
            "text": "The following is an example execution to provide better understanding.\nFirst change into the created python environment by typing  source malsami/bin/activate  You are now inside the python environment. (To leave it just type \u201cdeactivate\u201d)  Open up the interactive python shell  in the directory \u2018distributor\u2019 by typing (You can also create and execute this in a script)  sudo python3  sudo is necessary to allow for the creation and destruction of tap devices, via which the qemu instances are connected to the bridge.  Add appropriate imports:    (\u2018example.py\u2019 holds some tasksets for testing, \u2018loggingMonitor.py\u2019 is a  Monitor  for testing, which writes to logs/monitor.log)  from test import example5\nfrom distributor.distributorClass import Distributor\nfrom distributor.monitors.loggingMonitor import LoggingMonitor  Defining a monitor and tasksets for the execution is left to the user.  Initialize modules:  t = example5()\nlm = LoggingMonitor()\ndist = Distributor()  Now the distributor is running.  Adding a job  is possible via the add_jobs([taskset], monitor) function:   dist.add_job([t],lm)  This will spawn machines acording to the current max_machine value.  Note: You can repeat the above command to queue multiple jobs whenever you please.  To view a list of detached qemu instances   sudo screen -ls  To kill a detached screen type:  sudo screen -X -S <name\\_of\\_screen> kill  The log files of the genode instances are also saved in the distributor/log/ directory.  Additionally, you can adjust the number of spawned machines, also while the distributor is running. See the distributor functions for more information.",
            "title": "Using the Distributor"
        },
        {
            "location": "/data-generation/distributor/#distributor-functions",
            "text": "Setting max machines to a value between 1 and 42. You can change this anytime as this only affects the maximum total number of spawned machines. If machines are active, the number will be adapted accordingly. Closing machines will still finish their current taskset before shutting down. (The argument \u201cmax_allowed\u201d can be passed in the Distributors init() to adjust the limit.):      set_max_machine_value(numMachines)  Function to check if the distributor is busy or not:      get_distributor_state()  Return current maximum value of possible active machines:      get_max_machine_value()  Creating a new job and adding it to a list of jobs to be worked on.\nThe function is instantiating a new object of type _Job which is then appended to the list of jobs to be processed:      add_job(tasksetList, monitor=None, offset=0, *session_params)  A job always consists of list of TaskSet tasksetList and a Monitor implementing AbstractMonitor from monitor.py, the session_parameters are optional. Inside the method a _Job() will be created to hold the iterator over the list.  Kill all machines that are currently running  without  waiting for current tasksets to finish:      kill_all_machines()  Stop all machines after they finished the execution of their current taskset:      shut_down_all_machines()  Resume stopped machines:      resume()",
            "title": "Distributor functions"
        },
        {
            "location": "/data-generation/distributor/#the-machine-class",
            "text": "The \u2018machine.py\u2019 implements a class which extends threading.Thread.  An instance of Machine is taking care of spawning a host, creating a session which connects to the spawned host and acquiring tasksets while there still is work to be done.",
            "title": "The Machine class"
        },
        {
            "location": "/data-generation/monitor/",
            "text": "Monitor\n\u00b6\n\n\nThis section is supposed to provide insight on the monitor component and how it is intended to be used. The monitor has to be provided alongside the taskset component to the \nadd_jobs()\n function of the distributor. The monitors functions are called in \nrun()\n of the \nmachine.py\n.\n\n\nA monitor has to implement the \nAbstractMonitor\n in the distributor_service module which defines the following abstract methods:\n\n\n__taskset_event__(taskset, event)\n\u00b6\n\n\nThis function is called when the \nsession.run()\n as soon as new information about the taskset is returned from Genode.\n\n\n__taskset_start__(taskset)\n\u00b6\n\n\nThis function is called when a new taskset has benn started by calling the \nsession.start()\n function.\n\n\n__taskset_stop__(taskset)\n\u00b6\n\n\nThis function is not used from within the distributor component as of now.\n\n\n__taskset_finish__(self, taskset)\n\u00b6\n\n\nThis function is called when a taskset has been finished and \nsession.stop()\n has been called. If this function is called the monitor regards the taskset as completed and all information, which could be gained is available. \n\n\nIn general the monitor functions always hold a reference to the taskset the function was called with, so it has access to all information saved in the obect defined by the \ntask.py\n.",
            "title": "Monitor"
        },
        {
            "location": "/data-generation/monitor/#monitor",
            "text": "This section is supposed to provide insight on the monitor component and how it is intended to be used. The monitor has to be provided alongside the taskset component to the  add_jobs()  function of the distributor. The monitors functions are called in  run()  of the  machine.py .  A monitor has to implement the  AbstractMonitor  in the distributor_service module which defines the following abstract methods:",
            "title": "Monitor"
        },
        {
            "location": "/data-generation/monitor/#9595taskset95event9595taskset-event",
            "text": "This function is called when the  session.run()  as soon as new information about the taskset is returned from Genode.",
            "title": "__taskset_event__(taskset, event)"
        },
        {
            "location": "/data-generation/monitor/#9595taskset95start9595taskset",
            "text": "This function is called when a new taskset has benn started by calling the  session.start()  function.",
            "title": "__taskset_start__(taskset)"
        },
        {
            "location": "/data-generation/monitor/#9595taskset95stop9595taskset",
            "text": "This function is not used from within the distributor component as of now.",
            "title": "__taskset_stop__(taskset)"
        },
        {
            "location": "/data-generation/monitor/#9595taskset_finish9595self-taskset",
            "text": "This function is called when a taskset has been finished and  session.stop()  has been called. If this function is called the monitor regards the taskset as completed and all information, which could be gained is available.   In general the monitor functions always hold a reference to the taskset the function was called with, so it has access to all information saved in the obect defined by the  task.py .",
            "title": "__taskset_finish__(self, taskset)"
        },
        {
            "location": "/data-generation/datageneration/",
            "text": "Datageneration\n\u00b6\n\n\nThe implementation responsible for forming the tasks and as a result the tasksets for deployment is in the datageneration repository in the MaLSAMI project. The tasksets are incrementally created beginning with taksets of size one (each taskset consists of one task each) and then are continually created to build tasksets as big as size four (4 tasks per taskset). The progression works as follows:\n1. At first a certain amount of tasks(different parameter configurations) for each available task type are created.\n2. These tasks are then executed as tasksets of size one and categorized into successful (good) and unsuccessful (bad) tasksets.\n\n\nThe next steps are repeated after the tasksets of the current size are all executed:\n3. To create tasksets of size n, take all \u2018good\u2019 tasksets of size n-1 and combine them with each \u2018good\u2019 taskset of size 1.\n4. Then execute and categorize.\n\n\nCombining only tasksets that are known to be successful is meant to limit the combinatorial explosion. But even with this approach, numbers grow rapidly.\n\n\nParameters of our Data\n\u00b6\n\n\n    'PRIORITY': (1,5),\n    'DEADLINE' : (0, 0),\n    'PERIOD': (1,8),\n    'CRITICALTIME' : (0, 0),\n    'NUMBEROFJOBS': (1,8),\n    'OFFSET': (0,0),\n    'QUOTA': (100, 100),\n    'CAPS': (235, 235),\n    'CORES' : (0, 0),\n    'COREOFFSET' : (0, 0),\n    'ARG':\n            {'hey':(0,0),\n            'pi':(13,21),\n            'tumatmul':(12,19),\n            'cond_mod':(25,30)\n            }\n\n\n\n\nBecause we limit the PERIOD to whole seconds it is possible that tasks are blocking each other, which might not have happened if the value was less discrete.\n\n\nBelow is a brief description of the important files:\n\u00b6\n\n\ndata_parser.py\n\u00b6\n\n\nThe entire taskset data is stored in the log files for bookkeeping. For analysis, this file parses the log files in the chosen directory and stores into a databse. SQLite was used in this implementation but functionality for other mySQL databases should be possible as well. SQL is recommended because of the ease of querying and the connection between the databases. \n\n\nThe database consists of these three Tables: \n\n\n1] Job\n\n\n2] Task\n\n\n3] TaskSet\n\n\n1] Job table consists of all the jobs and its respective attributes. It also contains a column for the TaskID which referes to the Task that it was placed in and a SetID column which refers to the TaskSetID that it is a part of. \n\n\n2] The \nTask\n table consists of all the tasks that were deployed by the distributor. The attributes include the \u201cPriority\u201d, \u201cDeadline\u201d, and \u201cNumber of Jobs\u201d. It\u2019s primary key is its TaskID which is decided upon by the taskset. \n\n\n3] The \nTaskSet\n table consists of all the Tasksets and is primarily referenced through its SetID. It also contains the IDs of all the Tasks and has a column indicating if the respective TaskSet was successful or not. Each Taskset has \nn\n columns which corresponds to n Tasks in the respective TaskSet. For a TaskSet of size k < n. The first \nk\n columns each contain the respective TaskIDs and the remaining contain \n-1\n, indicating that the columns are empty. \n\n\nHere are some example queries to query the database for a dataset: \n\n\nGet TaskSets of size \n1\n: \n\n\nselect TaskSet.Set_ID, Task.*, TaskSet.Successful \n from Task \n inner join TaskSet on Task.Task_ID = TaskSet.TASK1_ID \n\n\n\n\nGet TaskSets of size \n3\n: \n\n\nsqlite3\n select TaskSet.Set_ID, t1.*, t2.*, t3.*, TaskSet.Successful '\n  from Task t1, Task t2, Task t3 \n  inner join TaskSet on t1.Task_ID = TaskSet.TASK1_ID \n  and \n  t2.Task_ID = TaskSet.TASK2_ID \n  and \n  t3.Task_ID = TaskSet.TASK3_ID \n  and TaskSet.TASK4_ID = -1\n\n\nNOTE\n: These quereies were performed for a SQLite database. Other databases may have differences.\n\n\nmake_tasks.py\n\u00b6\n\n\nThis calls a helper function in the \nparameter_config.py\n to generate a file per task type defined in the \nparameter_config.taskTypes\n. Each file consists of \nparameter_config.linesPerCall\n lines and each line is a list of hash values with \nparameter_config.tasksPerLine\n values.\n\n\nmain.py\n\u00b6\n\n\nThe \nmain.py\n can be called without argument, but for each task in \nparameter_config.taskTypes\n there has to exist a file (created by executing \nmake_tasks.py\n). \nmain.py\n can also be called with the argument \nc\n to continue the execution with data loaded from the following files: \nbad_tasksets\n, \ngood_tasksets\n and, if available, \npossible_tasksets\n. These contain information gained by a previous execution of \nmain.py\n.\n\n\nOther options: \n\n\nss\n - \u2018Show Status\u2019. Print current status of the data generation and the current length of all the task lists\n\n\nh\n - Halt Machines\n\n\nk\n - Kill all Machines \n\n\ns\n - Save current progress\n\n\nx\n - Clean exit \n\n\nHelper Functions\n\u00b6\n\n\nparameter_config.py\n\u00b6\n\n\nThis file contains all the parameters of the variables and data structure we use for the data generation. \n\n\nParameter are randomly chosen within a range of suitable values. (\u201cPriority\u201d: (1,5) indicates that a random task will be generated with a random value between 1 and 5. ) This file is referenced numerous time in the data creation while specifying parameters. \n\n\nFor easy book-keeping and facilitated reading and writing to file, the parameters are stored in a concatenated strings before the main task generation is made. This ensure uniqueness and also an easy reference to the tasks and the tasksets. \n\n\nvalue_init.py\n\u00b6\n\n\nThis file will initiate the values of the tasks according to the parameter values in parameter_config. This creates the Task object as a list of dictionaries. This package is called from make_takss.py and with the appropriate parameters which include the number of tasks to make and the package. Depending on the package chosen, the appropriate package will be populated with the selected number of tasks chosen. \n\n\nAdditionally, this file provides functions for plotting the distribution of the parameters of the tasksets.",
            "title": "Data Generation"
        },
        {
            "location": "/data-generation/datageneration/#datageneration",
            "text": "The implementation responsible for forming the tasks and as a result the tasksets for deployment is in the datageneration repository in the MaLSAMI project. The tasksets are incrementally created beginning with taksets of size one (each taskset consists of one task each) and then are continually created to build tasksets as big as size four (4 tasks per taskset). The progression works as follows:\n1. At first a certain amount of tasks(different parameter configurations) for each available task type are created.\n2. These tasks are then executed as tasksets of size one and categorized into successful (good) and unsuccessful (bad) tasksets.  The next steps are repeated after the tasksets of the current size are all executed:\n3. To create tasksets of size n, take all \u2018good\u2019 tasksets of size n-1 and combine them with each \u2018good\u2019 taskset of size 1.\n4. Then execute and categorize.  Combining only tasksets that are known to be successful is meant to limit the combinatorial explosion. But even with this approach, numbers grow rapidly.",
            "title": "Datageneration"
        },
        {
            "location": "/data-generation/datageneration/#parameters-of-our-data",
            "text": "'PRIORITY': (1,5),\n    'DEADLINE' : (0, 0),\n    'PERIOD': (1,8),\n    'CRITICALTIME' : (0, 0),\n    'NUMBEROFJOBS': (1,8),\n    'OFFSET': (0,0),\n    'QUOTA': (100, 100),\n    'CAPS': (235, 235),\n    'CORES' : (0, 0),\n    'COREOFFSET' : (0, 0),\n    'ARG':\n            {'hey':(0,0),\n            'pi':(13,21),\n            'tumatmul':(12,19),\n            'cond_mod':(25,30)\n            }  Because we limit the PERIOD to whole seconds it is possible that tasks are blocking each other, which might not have happened if the value was less discrete.",
            "title": "Parameters of our Data"
        },
        {
            "location": "/data-generation/datageneration/#below-is-a-brief-description-of-the-important-files",
            "text": "",
            "title": "Below is a brief description of the important files:"
        },
        {
            "location": "/data-generation/datageneration/#data_parserpy",
            "text": "The entire taskset data is stored in the log files for bookkeeping. For analysis, this file parses the log files in the chosen directory and stores into a databse. SQLite was used in this implementation but functionality for other mySQL databases should be possible as well. SQL is recommended because of the ease of querying and the connection between the databases.   The database consists of these three Tables:   1] Job  2] Task  3] TaskSet  1] Job table consists of all the jobs and its respective attributes. It also contains a column for the TaskID which referes to the Task that it was placed in and a SetID column which refers to the TaskSetID that it is a part of.   2] The  Task  table consists of all the tasks that were deployed by the distributor. The attributes include the \u201cPriority\u201d, \u201cDeadline\u201d, and \u201cNumber of Jobs\u201d. It\u2019s primary key is its TaskID which is decided upon by the taskset.   3] The  TaskSet  table consists of all the Tasksets and is primarily referenced through its SetID. It also contains the IDs of all the Tasks and has a column indicating if the respective TaskSet was successful or not. Each Taskset has  n  columns which corresponds to n Tasks in the respective TaskSet. For a TaskSet of size k < n. The first  k  columns each contain the respective TaskIDs and the remaining contain  -1 , indicating that the columns are empty.   Here are some example queries to query the database for a dataset:   Get TaskSets of size  1 :   select TaskSet.Set_ID, Task.*, TaskSet.Successful \n from Task \n inner join TaskSet on Task.Task_ID = TaskSet.TASK1_ID   Get TaskSets of size  3 :   sqlite3\n select TaskSet.Set_ID, t1.*, t2.*, t3.*, TaskSet.Successful '\n  from Task t1, Task t2, Task t3 \n  inner join TaskSet on t1.Task_ID = TaskSet.TASK1_ID \n  and \n  t2.Task_ID = TaskSet.TASK2_ID \n  and \n  t3.Task_ID = TaskSet.TASK3_ID \n  and TaskSet.TASK4_ID = -1  NOTE : These quereies were performed for a SQLite database. Other databases may have differences.",
            "title": "data_parser.py"
        },
        {
            "location": "/data-generation/datageneration/#make95taskspy",
            "text": "This calls a helper function in the  parameter_config.py  to generate a file per task type defined in the  parameter_config.taskTypes . Each file consists of  parameter_config.linesPerCall  lines and each line is a list of hash values with  parameter_config.tasksPerLine  values.",
            "title": "make_tasks.py"
        },
        {
            "location": "/data-generation/datageneration/#mainpy",
            "text": "The  main.py  can be called without argument, but for each task in  parameter_config.taskTypes  there has to exist a file (created by executing  make_tasks.py ).  main.py  can also be called with the argument  c  to continue the execution with data loaded from the following files:  bad_tasksets ,  good_tasksets  and, if available,  possible_tasksets . These contain information gained by a previous execution of  main.py .  Other options:   ss  - \u2018Show Status\u2019. Print current status of the data generation and the current length of all the task lists  h  - Halt Machines  k  - Kill all Machines   s  - Save current progress  x  - Clean exit",
            "title": "main.py"
        },
        {
            "location": "/data-generation/datageneration/#helper-functions",
            "text": "",
            "title": "Helper Functions"
        },
        {
            "location": "/data-generation/datageneration/#parameter95configpy",
            "text": "This file contains all the parameters of the variables and data structure we use for the data generation.   Parameter are randomly chosen within a range of suitable values. (\u201cPriority\u201d: (1,5) indicates that a random task will be generated with a random value between 1 and 5. ) This file is referenced numerous time in the data creation while specifying parameters.   For easy book-keeping and facilitated reading and writing to file, the parameters are stored in a concatenated strings before the main task generation is made. This ensure uniqueness and also an easy reference to the tasks and the tasksets.",
            "title": "parameter_config.py"
        },
        {
            "location": "/data-generation/datageneration/#value95initpy",
            "text": "This file will initiate the values of the tasks according to the parameter values in parameter_config. This creates the Task object as a list of dictionaries. This package is called from make_takss.py and with the appropriate parameters which include the number of tasks to make and the package. Depending on the package chosen, the appropriate package will be populated with the selected number of tasks chosen.   Additionally, this file provides functions for plotting the distribution of the parameters of the tasksets.",
            "title": "value_init.py"
        },
        {
            "location": "/data-generation/data-visualization/",
            "text": "Data Visualizations\n\u00b6\n\n\nTo get an idea of how the data is distributed, we displayed the data and its hyperparameters using different visualization plots:\n\n\nHere is an example of the first ten lines of the Task Table\n\n\n\n\nEach task has a \u201cNumber_of_Jobs\u201d. Although the number of jobs is used as a parameter in the learning process, the tasks that are being deplayed all consists of Jobs. These Jobs are represented in the table below\n\n\n\n\n\n\n\n\n\nDistribution of the main learning parameters are listed below:",
            "title": "Data Visualization"
        },
        {
            "location": "/data-generation/data-visualization/#data-visualizations",
            "text": "To get an idea of how the data is distributed, we displayed the data and its hyperparameters using different visualization plots:  Here is an example of the first ten lines of the Task Table   Each task has a \u201cNumber_of_Jobs\u201d. Although the number of jobs is used as a parameter in the learning process, the tasks that are being deplayed all consists of Jobs. These Jobs are represented in the table below     \nDistribution of the main learning parameters are listed below:",
            "title": "Data Visualizations"
        },
        {
            "location": "/about/",
            "text": "Goal of the project\n\u00b6\n\n\nThis project is about schedulability analysis.",
            "title": "About"
        },
        {
            "location": "/about/#goal-of-the-project",
            "text": "This project is about schedulability analysis.",
            "title": "Goal of the project"
        }
    ]
}